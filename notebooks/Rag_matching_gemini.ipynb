{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ab91de9-4db7-432a-bd64-909fb8c7ebf1",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import PyPDF2\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "import spacy\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "import time\n",
    "from google import genai\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from google.genai import types\n",
    "\n",
    "class GeminiEmbeddings(Embeddings):\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        result = client.models.embed_content(\n",
    "            model=\"gemini-embedding-exp-03-07\",\n",
    "            contents=text\n",
    "        )\n",
    "        return result.embeddings[0].values\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "\n",
    "txt_folder = \"Enter your file path here\"\n",
    "gemini_api_key_lab=\"Enter your API key here\"\n",
    "client = genai.Client(api_key=gemini_api_key_lab)\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def split_sentences_with_spacy(text):\n",
    "    \"\"\"Use spaCy to split text into sentences.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "def extract_text_from_pdf(pdf_folder):\n",
    "    \"\"\"Extract text excluding references from a PDF file.\"\"\"\n",
    "    \n",
    "    with open(pdf_folder, \"rb\") as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\".join(page.extract_text() for page in reader.pages)\n",
    "        \n",
    "        # Find where references begin (common markers)\n",
    "        ref_markers = [\"references\",\"References\", \"Acknowledgements\", \"REFERENCES\"]\n",
    "        ref_start = float('inf')\n",
    "        for marker in ref_markers:\n",
    "            pos = text.rfind(marker) #find all and pinpoint the last one\n",
    "            if pos != -1:\n",
    "                ref_start = min(ref_start, pos)\n",
    "                \n",
    "        # Return text up to references section\n",
    "        if ref_start != float('inf'):\n",
    "            return text[:ref_start]\n",
    "        return text\n",
    "    \n",
    "def extract_text_from_txt(txt_folder):\n",
    "    \"\"\"Extract text excluding references from a .txt file.\"\"\"\n",
    "    with open(txt_folder, \"r\", encoding='UTF-8') as txt_file:\n",
    "        lines = txt_file.readlines()\n",
    "        text = \"\".join(line for line in lines)\n",
    "        \n",
    "        # Find where references begin (common markers)\n",
    "        ref_markers = [\"references\",\"References\", \"Acknowledgements\", \"REFERENCES\"]\n",
    "        ref_start = float('inf')\n",
    "        for marker in ref_markers:\n",
    "            pos = text.rfind(marker) #find all and pinpoint the last one\n",
    "            if pos != -1:\n",
    "                ref_start = min(ref_start, pos)\n",
    "                \n",
    "        # Return text up to references section\n",
    "        if ref_start != float('inf'):\n",
    "            return text[:ref_start]\n",
    "        return text\n",
    "\n",
    "\n",
    "def extract_numbers(file_name):\n",
    "    \"\"\"Extract numeric values from a file name.\"\"\"\n",
    "    match = re.match(r\"(\\d+)-(\\d+)\", file_name)  \n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2))  \n",
    "    return float('inf'), float('inf') \n",
    "\n",
    "def process_reference_papers(pdf_folder):\n",
    "    \"\"\"Process all PDF files in the folder and create documents.\"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(pdf_folder, filename)\n",
    "            print(filename)\n",
    "            text = extract_text_from_pdf(filepath)\n",
    "            # Exclude reference lists if needed \n",
    "            documents.append(Document(page_content=text, metadata={\"source\": filename}))\n",
    "\n",
    "def process_reference_papers_txt(txt_folder):\n",
    "    \"\"\"Process all PDF files in the folder and create documents.\"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(txt_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(txt_folder, filename)\n",
    "            print(filename)\n",
    "            text = extract_text_from_txt(filepath)\n",
    "            # Exclude reference lists if needed \n",
    "            documents.append(Document(page_content=text, metadata={\"source\": filename}))\n",
    "    return documents\n",
    "\n",
    "def preprocess_introduction(intro_text):\n",
    "\n",
    "    ref_pattern = r\"\\[(\\d+(?:[-\\u2013]\\d+)?(?:,\\s*\\d+(?:[-\\u2013]\\d+)?)*)\\]\"\n",
    "    \n",
    "    def expand_range(range_str):\n",
    "        numbers = set()\n",
    "\n",
    "        parts = range_str.replace('\\u2013', '-').replace(' ', '').split(',')\n",
    "        for part in parts:\n",
    "            if '-' in part:\n",
    "                start, end = map(int, part.split('-'))\n",
    "                numbers.update(range(start, end + 1))\n",
    "            else:\n",
    "                numbers.add(int(part))\n",
    "        return sorted(numbers)\n",
    "    \n",
    "    matches = re.finditer(ref_pattern, intro_text)\n",
    "    ground_truth = {}\n",
    "    ref_count = 1\n",
    "    \n",
    "    cleaned_intro = intro_text\n",
    "    for match in matches:\n",
    "        original = match.group(0)\n",
    "        numbers = expand_range(match.group(1))\n",
    "        placeholder = f\"[ref{ref_count}]\"\n",
    "        ground_truth[placeholder] = numbers\n",
    "        cleaned_intro = cleaned_intro.replace(original, placeholder, 1)\n",
    "        ref_count += 1\n",
    "        \n",
    "    return cleaned_intro, ground_truth\n",
    "\n",
    "# Utility function for cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "\n",
    "    vec1 = np.array(vec1).flatten()  \n",
    "    vec2 = np.array(vec2).flatten()  \n",
    "    \n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    if norm_product == 0:\n",
    "        return 0\n",
    "    return np.dot(vec1, vec2) / norm_product\n",
    "\n",
    "# Save and Load Embeddings\n",
    "def save_embeddings(documents, file_name=\".pkl\"): #embedding file name insert\n",
    "    \"\"\"Save embeddings to a file.\"\"\"\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(documents, f)\n",
    "\n",
    "def load_embeddings(file_name=\".pkl\"): #embedding file name insert\n",
    "    \"\"\"Load embeddings from a file.\"\"\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Phase 2: Build Vector Store\n",
    "def build_vector_store(documents, embedding_file=\".pkl\", chunk_size=1000, chunk_overlap=200):  #embedding file name insert\n",
    "    \"\"\"Create a FAISS vector store from documents with caching for Gemini embeddings.\"\"\"\n",
    "    try:\n",
    "        # Try to load split_documents with pre-computed embeddings\n",
    "        split_documents = load_embeddings(os.path.join(txt_folder , embedding_file))\n",
    "        print(\" Loaded embeddings from file.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\" Embeddings file not found. Creating embeddings...\")\n",
    "        \n",
    " \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "        split_documents = split_documents[:]\n",
    "\n",
    "        for idx, doc in enumerate(split_documents):\n",
    "            try:\n",
    "                result = client.models.embed_content(\n",
    "                    model=\"gemini-embedding-exp-03-07\",\n",
    "                    contents=doc.page_content\n",
    "                )\n",
    "                embed_ = result.embeddings[0].values  \n",
    "                doc.metadata[\"embedding\"] = embed_\n",
    "                print(f\" Generated embedding for chunk {idx + 1}/{len(split_documents)}\")\n",
    "                time.sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\" Error embedding document {idx + 1}: {e}\")\n",
    "                doc.metadata[\"embedding\"] = [0.0] * 768  \n",
    "\n",
    "\n",
    "        save_embeddings(split_documents, embedding_file)\n",
    "\n",
    "    texts = [doc.page_content for doc in split_documents]\n",
    "    metadatas = [doc.metadata for doc in split_documents]\n",
    "    embeddings = [doc.metadata[\"embedding\"] for doc in split_documents]\n",
    "    \n",
    "    \n",
    "    text_embedding_pairs = list(zip(texts, embeddings))\n",
    "    \n",
    "    embedding_model = GeminiEmbeddings()\n",
    "    \n",
    "    vector_store = FAISS.from_embeddings(\n",
    "        text_embeddings=text_embedding_pairs,\n",
    "        embedding=embedding_model,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "   \n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\" Total documents in vector store:\", vector_store.index.ntotal)\n",
    "    unique_sources = {doc.metadata.get(\"source\", \"N/A\") for doc in split_documents}\n",
    "    print(\" Unique document sources:\", len(unique_sources))\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "# Phase 3: Reference Prediction\n",
    "def predict_references_with_RAG(vector_store, cleaned_intro):\n",
    "    \"\"\"Predict references for each [ref] in the introduction using RAG\"\"\"\n",
    "    faiss_references_dict = {}\n",
    "\n",
    "    for placeholder in re.findall(r\"\\[ref\\d+\\]\", cleaned_intro):\n",
    "        query_text = extract_context_for_ref(cleaned_intro, placeholder, sentences_before=1, sentences_after=0)\n",
    "\n",
    "        all_documents = vector_store.similarity_search(query_text, k=60)\n",
    "              \n",
    "        unique_references = []\n",
    "        seen = set()\n",
    "        \n",
    "        for doc in all_documents:\n",
    "            source = doc.metadata[\"source\"]\n",
    "            if source not in seen and len(unique_references) < 5:\n",
    "                unique_references.append(source)\n",
    "                seen.add(source)\n",
    "        \n",
    "        faiss_references_dict[placeholder] = unique_references\n",
    "        print(f\"FAISS references for {placeholder}: {unique_references}\")\n",
    "\n",
    "    return faiss_references_dict\n",
    "\n",
    "# Placeholder context extraction\n",
    "def extract_context_for_ref(cleaned_intro, placeholder, sentences_before=1, sentences_after=0):\n",
    "    \"\"\"Extract a window of sentences around [ref] for focused queries.\"\"\"\n",
    "    sentences = split_sentences_with_spacy(cleaned_intro)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if placeholder in sentence:\n",
    "            start = max(0, i - sentences_before)\n",
    "            end = min(len(sentences), i + sentences_after + 1)\n",
    "            context = '. '.join(sentences[start:end]).strip()\n",
    "            return context\n",
    "    print(f\"Warning: Placeholder {placeholder} not found in any sentence.\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_predictions(ground_truth, faiss_references_dict):  \n",
    "    \"\"\"Evaluate predictions against ground truth and calculate the overall score.\"\"\"\n",
    "    total_matched_refs = 0\n",
    "    total_ground_truth_refs = 0\n",
    "    detailed_report = []\n",
    "\n",
    "    for placeholder, true_refs in ground_truth.items():\n",
    "        predicted_refs = faiss_references_dict.get(placeholder, [])\n",
    "        \n",
    "        true_refs = {ref for ref in true_refs}\n",
    "        predicted_refs = {int(ref.split(\"-\")[-1].replace(\".txt\", \"\")) for ref in predicted_refs}\n",
    "\n",
    "        # Count matched references directly\n",
    "        total_matched_refs += len(set(predicted_refs) & set(true_refs))\n",
    "        total_ground_truth_refs += len(true_refs)\n",
    "        \n",
    "        detailed_report.append({\n",
    "            \"Placeholder\": placeholder,\n",
    "            \"Ground Truth\": true_refs,\n",
    "            \"predicted_refs\": predicted_refs\n",
    "        })\n",
    "\n",
    "    # Calculate overall score as total matched references / total ground truth references\n",
    "    overall_score = total_matched_refs / total_ground_truth_refs if total_ground_truth_refs > 0 else 0\n",
    "\n",
    "    return detailed_report, overall_score\n",
    "\n",
    "\n",
    "# Phase 7: Main Execution\n",
    "\n",
    "# Step 1: Process reference papers\n",
    "print(\"Processing reference papers...\")\n",
    "documents = process_reference_papers_txt(txt_folder)\n",
    "\n",
    "# Step 2: Build vector store\n",
    "print(\"Building vector store...\")\n",
    "vector_store = build_vector_store(documents)\n",
    "\n",
    "# Step 3: Input introduction section\n",
    "intro_text = \"\"\"     The term “globalisation ” is often used in a similar way , indeed almost interchangeably \n",
    ",to internationalisation when discussing higher education policy and\n",
    "trends.Some authors,however,seek to make a distinction between the two,seeing\n",
    "internationalisation as a contemporary expression of internationalism ,\n",
    "encompassing responses to the “forces ” of globalisation , which are viewed as\n",
    "being far from benign in nature[6,25,29,70].\n",
    "Inpractice, some limit-ations are normally placed upon the search,such asinterms of the date,\n",
    "place , and language of publication . In this case, the search was limited to items\n",
    "published inthe English language ,and there was aparticular focus on most recent\n",
    "publications , given that a related systematic review[73] had recently been undertaken.\n",
    "The items identi ﬁed were\n",
    "then checked for relevance ;where relevant ,copies were obtained for scrutiny and\n",
    "analysis,with any additional items identiﬁed through the irreferencesfol-lowedup.\n",
    "The analysis presented in this article builds on and extends an earlier review\n",
    "of research on globalisation and internationalisation in higher education[73].\n",
    "Related analyses have also been carried out by others: for example,Kos-mützkyand\n",
    "Putty ’s review of the literature on transnational ,oﬀshore ,cross -border ,\n",
    "and borderless higher education[46]; Bedenlier et al.’s examination of 2\n",
    "decades ofresearch into the internationalisation of higher education published in\n",
    "the Journal of Studies in International Education [11];\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "cleaned_intro, ground_truth = preprocess_introduction(intro_text)\n",
    "\n",
    "# Step 4: Predict references\n",
    "print(\"Predicting references...\")\n",
    "faiss_references_dict = predict_references_with_RAG(vector_store, cleaned_intro)\n",
    "\n",
    "# Step 5: Assess predictions (if in assessment mode)\n",
    "print(\"Assessing predictions...\")\n",
    "detailed_report, overall_score = evaluate_predictions(ground_truth, faiss_references_dict)\n",
    "\n",
    "# Output results\n",
    "print(\"\\nDetailed Report:\")\n",
    "for item in detailed_report:\n",
    "    print(item)\n",
    "print(f\"\\noverall_score: {overall_score*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce122ac-b400-4f88-b959-13a79dd089df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
